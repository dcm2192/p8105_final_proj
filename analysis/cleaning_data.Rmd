---
title: "Data Cleaning"
author: "Wayne Monical"
date: "2024-11-16"
output: html_document
---


```{r}
library(dplyr)
library(tidyverse)
library(stringr)
library(raster)
library(raster)
library(sf)
library(terra) 
```


## Cleaning Elk Data

We drop the `tz` timezone variable, because it is homogenous. We drop the `utm_x` and `utm_y` variables because they are simply another kind of location tracking, and we already have latitude and longitude. Note that [only elk that migrated from the Elk reserve to Yellowstone were included in the data](https://www.sciencebase.gov/catalog/file/get/5a9f2782e4b0b1c392e502ea?f=__disk__27%2F1f%2Faa%2F271faa32dbaff38082fc4661db798fcd74908130&transform=1&allowOpen=true).



```{r}
elk = 
  read_csv('../raw_data/Elk GPS collar data from National Elk Refuge 2006-2015.csv') |> 
  janitor::clean_names() |> 
  dplyr::mutate(
    day = day(dt),
    hour = hour(dt),
    dist_km = 
      ifelse(
        elk_id == lag(elk_id),
        geosphere::distHaversine(cbind(long, lat), cbind(lag(long), lag(lat)))/1000,
        NA)
    ) |> 
  dplyr::select(
    elk_id,
    dt,
    year, 
    month,
    day,
    hour, 
    lat,
    long,
    dist_km
  )
```

```{r}
# looks like we'v esolved the weird december data
elk |> 
  drop_na(dist_km) |> 
  ggplot(aes(x = factor(month), y = dist_km)) + geom_violin()
```

## Land Cover Cleaning

Using the [terra package](https://bookdown.org/mcwimberly/gdswr-book/coordinate-reference-systems.html#reprojecting-raster-data)
```{r, eval=FALSE}
land = rast('../raw_data/land_cover/land_cover.tif')

# Reprojecting in latitude and longitude
land_coord = project(land, "EPSG:4326")

plot(land_coord)

# Subset to the relevant area
min_long = elk |> pull(long) |> min()
max_long = elk |> pull(long) |> max()
rng_long = abs(min_long - max_long)
lowerleftlon = min_long - 0.1 * rng_long 
upperrightlon = max_long + 0.1 * rng_long
min_lat = elk |> pull(lat) |> min()
max_lat = elk |> pull(lat) |> max()
rng_lat = abs(min_lat - max_lat)
lowerleftlat = min_lat - 0.1 * rng_lat
upperrightlat = max_lat + 0.1 * rng_lat


# cropping 
small_land_coord = crop(
  land_coord, 
  extent(lowerleftlon, upperrightlon, lowerleftlat, upperrightlat))

plot(small_land_coord)

# writing raster
# terra::writeRaster(small_land_coord, '../clean_data/land_cover.tif')
```


Read in saved raster
```{r}
small_land_coord = rast('../clean_data/land_cover.tif')
```

Convert to points
```{r}
land_coord_df = as.points(small_land_coord)
```

Get the land cover at the relevent points of the analysis
```{r}
temp_elk = 
  elk |> 
  mutate(
    longitude = long,
    latitude  = lat) |> 
  dplyr::select(
    longitude,
    latitude
  )

elk_land_cover = terra::extract(x = small_land_coord, y = temp_elk)
```

Add in the data
```{r}
elk = 
  elk |> 
  mutate(land_cover = elk_land_cover$land_cover)
```

Saving elk data
```{r}
# elk |> write.csv('../clean_data/elk.csv', row.names =  FALSE)

# read in the previously saved elk data
elk = read_csv('../clean_data/elk.csv')
```

## Cleaning Water Quality Data

### Locations

This data set gives the locations that water was sampled from. We drop the `org_code` variable because it is homogenous
```{r}
water_quality_locations = 
  read_csv('../raw_data/water_quality/Locations.csv')|> 
  janitor::clean_names() |> 
  dplyr::select(
    location_id,
    location_name,
    park_code, 
    location_type,
    latitude,
    longitude
  )
```


### Results

```{r}
water_quality_results = 
  read_csv('../raw_data/water_quality/Results.csv')|> 
  janitor::clean_names()
```

We find the most common observations.
```{r}
common_obs = 
  water_quality_results |> 
  drop_na(result_text) |> 
  group_by(characteristic_name) |> 
  summarize(n = n())|> 
  arrange(desc(n)) |> 
  filter(
    n > 875, 
    characteristic_name != "Weather Comments (text)") |> 
  pull(characteristic_name)
```

We filter for the most common observations. We filter for readings that we can use, given in the `acceptable_readings` variable. We replace non-detected values with zero. We select for the relevant columns 
```{r}

acceptable_readings = c("Detected and Quantified", "Not Detected", "Present Below Quantification Limit")

water_quality_results=
  water_quality_results |> 
  filter(
    characteristic_name %in% common_obs,
    result_detection_condition %in% acceptable_readings) |> 
  mutate(
    result_text = stringr::str_replace(result_text, "NULL", "0"),
    result_unit = stringr::str_replace(result_unit, "None", ""),
    characteristic_name = paste0(characteristic_name, " ", result_unit) |> trimws(),
    year = year(activity_start_date),
    month = month(activity_start_date),
    day = day(activity_start_date)
    ) |> 
  dplyr::select(
    location_id,
    activity_id,
    activity_type,
    activity_start_date,
    year,
    month,
    day,
    characteristic_name,
    result_text
  ) 
```





### Combining

```{r}
water_quality = 
  water_quality_locations |> 
  left_join(water_quality_results)

# water_quality |> write.csv('../clean_data/water_quality.csv', row.names = FALSE)
```

```{r}
water_quality
```


```{r}
water_quality %>% 
  dplyr::select(
    location_id,
    longitude,
    latitude
  ) %>%
  distinct() %>% 
  arrange(desc(latitude))
```




```{r}
water_quality %>% 
  dplyr::select(
    location_id,
    longitude,
    latitude
  ) %>% 
  ggplot(aes(x = longitude, y = latitude))+
  geom_point() + 
  geom_point(
    data = elk,
    aes(x = long, y = lat, color = 'red'), alpha = 0.01
  )
```



```{r}
min(water_quality$latitude)
max(water_quality$latitude)
min(water_quality$longitude)
max(water_quality$longitude)

```

## Importing Yellowstone Peak Data


[Definitions Pg 3](https://files.cfc.umt.edu/cesu/NPS/CSU/2010/10Stevens_ROMN_alpine%20monitoring_rpt.pdf)

```{r}
summit = tibble(
  summit_code = c('BLP', 'SCP', 'SPP', 'WSP'),
  name = c('Boundary Line Peak', 'Stone Crop Peak',
         'Snow Pipit Peak', 'Wolf Scat Peak'),
  lat = c(44.70028, 44.69584, 44.70167, 44.69444), 
  long = c(-109.8267, -109.8339, -109.835, -109.8364),
  elevation_in_meters =  c(3195, 3122, 3169, 3124)
  )
```





## Cleaning Soil Chemistry 

What do these abbreviations mean? [Here](https://files.cfc.umt.edu/cesu/NPS/CSU/2010/10Stevens_ROMN_alpine%20monitoring_rpt.pdf)

Boundary Line Peak (BLP), Stone Crop Peak (SCP), Snow Pipit Peak (SPP), and Wolf
Scat Peak (WSP)

```{r}
soil_chem = 
  read_csv('../raw_data/NPS_IMD_GLORIA_SoilChemistry_2287252_DataPackage/NPS_IMD_GLORIA_SoilChemistry_2287252-dataset.csv') |> 
  janitor::clean_names()

soil_chem= 
  soil_chem |> 
  filter(
    ! average %in% c('Insufficient sample', 'insufficient sample'),
    stringr::str_detect(site_name, "YNP")
  ) |> 
  mutate(
    year = year(start_date),
    month = month(start_date),
    day = day(start_date),
    parameter = paste0(parameter_dataset, " ", unit_dataset),
    summit_code = str_sub(site_name, start = 8, end = 10)
  ) |> 
  inner_join(summit) |> 
 dplyr::select(
    summit_code,
    name,
    lat,
    long,
    elevation_in_meters,
    site_name,
    event_name,
    year,
    month,
    day,
    parameter,
    average)
  

soil_chem |> write.csv('../clean_data/soil_chem.csv', row.names = FALSE)
```






## Cleaning Soil Temperature

Read in data, filter for yellowstone national park, 
```{r}
soil_temp = 
  read_csv('../raw_data/NPS_IMD_GLORIA_SoilTemperature_2288176_DataPackage/NPS_IMD_GLORIA_SoilTemperature_2288176_Daily-dataset.csv') |> 
  janitor::clean_names()  |> 
  filter(park == 'YELL') |> 
  mutate(
    summit_code = summit,
    year = year(date_time),
    month = month(date_time),
    day = day(date_time))
```

```{r}
soil_temp = 
  soil_temp |>
  inner_join(summit) |> 
  select(
    summit_code,
    name,
    lat,
    long,
    elevation_in_meters,
    plot, 
    site_name,
    year,
    month,
    day,
    daily_mean,
    daily_standard_dev
  )

soil_temp |> write.csv('../clean_data/soil_temp.csv', row.names = FALSE)
```


## Reading in Temperature

Filter for the four closest stations, filter for the correct date range, average the temperature and snowfall across the stations. 

```{r}
# closest four stations
four_stations <- 
  c("SNAKE RIVER STATION, WY US", "MORAN 5 WNW, WY US", "BURRO HILL WYOMING, WY US", "MOOSE 1 NNE, WY US")

weather = read_csv("../raw_data/raw_weather_data.csv") |> 
  janitor::clean_names() |> 
  filter(
    name %in% four_stations,
    date >= '2006-03-01', 
    date <= '2015-08-25') |> 
  group_by(date) |> 
  summarize(
    tavg = mean(tavg, na.rm = TRUE),
    tmin = mean(tmin, na.rm = TRUE),
    tmax = mean(tmin, na.rm = TRUE),
    prcp = mean(prcp, na.rm = TRUE),
    snow = mean(snow, na.rm = TRUE),
    snwd = mean(snwd, na.rm = TRUE),
  ) |> 
  mutate(
    year = year(date),
    month = month(date),
    day = day(date)
  )
```




## Combining all data


```{r}
head(elk)
```

```{r}
head(weather)
```

```{r}
head(water_quality)
```

```{r}
table(water_quality$characteristic_name)
```

```{r}
water_quality |> 
  group_by('location_id', 'location_name', 'latitude', 'longitude', 'activity_start_date', 'year', 'month', 'day', 'characteristic_name')
    
```




```{r}
chars = c('pH')


water_quality |> 
  # filter(characteristic_name  %in% chars) |> 
   #dplyr::select(-activity_type) |>
  # distinct() |> 
  pivot_wider(
    id_cols = c('location_id', 'location_name', 'latitude', 'longitude', 'activity_start_date', 'year', 'month', 'day'),
    names_from = 'characteristic_name',
    values_from = 'result_text'
  ) |> 
  unnest(cols = unique(water_quality$characteristic_name))
```


```{r}
all = left_join(elk, weather |> dplyr::select(-date))
```

```{r}
all |> write.csv('../clean_data/all_data.csv', row.names =  FALSE)
```


