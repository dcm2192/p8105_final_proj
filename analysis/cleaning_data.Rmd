---
title: "Data Cleaning"
author: "Wayne Monical"
date: "2024-11-16"
output: html_document
---

Library load
```{r}
library(dplyr)
library(tidyverse)
library(stringr)
library(raster)
library(raster)
library(sf)
library(terra) 
library(geosphere)
```


## Cleaning Elk Data

We drop the `tz` timezone variable, because it is homogenous. We drop the `utm_x` and `utm_y` variables because they are simply another kind of location tracking, and we already have latitude and longitude. Note that [only elk that migrated from the Elk reserve to Yellowstone were included in the data](https://www.sciencebase.gov/catalog/file/get/5a9f2782e4b0b1c392e502ea?f=__disk__27%2F1f%2Faa%2F271faa32dbaff38082fc4661db798fcd74908130&transform=1&allowOpen=true).



```{r}
elk = 
  read_csv('../raw_data/Elk GPS collar data from National Elk Refuge 2006-2015.csv') |> 
  janitor::clean_names() |> 
  dplyr::mutate(
    day = day(dt),
    hour = hour(dt),
    dist_km = 
      ifelse(
        elk_id == lag(elk_id),
        geosphere::distHaversine(cbind(long, lat), cbind(lag(long), lag(lat)))/1000,
        NA)
    ) |> 
  dplyr::select(
    elk_id,
    dt,
    year, 
    month,
    day,
    hour, 
    lat,
    long,
    dist_km
  )
```

```{r}
# looks like we'v esolved the weird december data
elk |> 
  drop_na(dist_km) |> 
  ggplot(aes(x = factor(month), y = dist_km)) + geom_violin()
```

## Land Cover Cleaning

Using the [terra package](https://bookdown.org/mcwimberly/gdswr-book/coordinate-reference-systems.html#reprojecting-raster-data)
```{r, eval=FALSE}
land = rast('../raw_data/land_cover/land_cover.tif')

# Reprojecting in latitude and longitude
land_coord = project(land, "EPSG:4326")

plot(land_coord)

# Subset to the relevant area
min_long = elk |> pull(long) |> min()
max_long = elk |> pull(long) |> max()
rng_long = abs(min_long - max_long)
lowerleftlon = min_long - 0.1 * rng_long 
upperrightlon = max_long + 0.1 * rng_long
min_lat = elk |> pull(lat) |> min()
max_lat = elk |> pull(lat) |> max()
rng_lat = abs(min_lat - max_lat)
lowerleftlat = min_lat - 0.1 * rng_lat
upperrightlat = max_lat + 0.1 * rng_lat


# cropping 
small_land_coord = crop(
  land_coord, 
  extent(lowerleftlon, upperrightlon, lowerleftlat, upperrightlat))

plot(small_land_coord)

# writing raster
# terra::writeRaster(small_land_coord, '../clean_data/land_cover.tif')
```


Read in saved raster
```{r}
small_land_coord = rast('../clean_data/land_cover.tif')
```

Convert to points
```{r}
land_coord_df = as.points(small_land_coord)
```

```{r}
nrow(land_coord_df)
```
```{r}
land_coord_df$
```


```{r}
head(land_coord_df)
```


Get the land cover at the relevent points of the analysis
```{r}
temp_elk = 
  elk |> 
  mutate(
    longitude = long,
    latitude  = lat) |> 
  dplyr::select(
    longitude,
    latitude
  )

elk_land_cover = terra::extract(x = small_land_coord, y = temp_elk)
```

Add in the data
```{r}
elk = 
  elk |> 
  mutate(land_cover = elk_land_cover$land_cover)
```

Saving elk data
```{r}
elk |> write.csv('../clean_data/elk.csv', row.names =  FALSE)
```

## Cleaning Water Quality Data

### Locations

This data set gives the locations that water was sampled from. We drop the `org_code` variable because it is homogenous
```{r}
water_quality_locations = 
  read_csv('../raw_data/water_quality/Locations.csv')|> 
  janitor::clean_names() |> 
  dplyr::select(
    location_id,
    location_name,
    park_code, 
    location_type,
    latitude,
    longitude
  )
```


### Results

```{r}
water_quality_results = 
  read_csv('../raw_data/water_quality/Results.csv')|> 
  janitor::clean_names()
```

We find the most common observations.
```{r}
common_obs = 
  water_quality_results |> 
  drop_na(result_text) |> 
  group_by(characteristic_name) |> 
  summarize(n = n())|> 
  arrange(desc(n)) |> 
  filter(
    n > 875, 
    characteristic_name != "Weather Comments (text)") |> 
  pull(characteristic_name)
```

We filter for the most common observations. We filter for readings that we can use, given in the `acceptable_readings` variable. We replace non-detected values with zero. We select for the relevant columns 
```{r}

acceptable_readings = c("Detected and Quantified", "Not Detected", "Present Below Quantification Limit")

water_quality_results=
  water_quality_results |> 
  filter(
    characteristic_name %in% common_obs,
    result_detection_condition %in% acceptable_readings) |> 
  mutate(
    result_text = stringr::str_replace(result_text, "NULL", "0"),
    result_unit = stringr::str_replace(result_unit, "None", ""),
    characteristic_name = paste0(characteristic_name, " ", result_unit) |> trimws(),
    year = year(activity_start_date),
    month = month(activity_start_date),
    day = day(activity_start_date)
    ) |> 
  dplyr::select(
    location_id,
    activity_id,
    activity_type,
    activity_start_date,
    year,
    month,
    day,
    characteristic_name,
    result_text
  ) 
```



### Combining Water Data

```{r}
water_quality = 
  water_quality_locations |> 
  left_join(water_quality_results)

water_quality |> write.csv('../clean_data/water_quality.csv', row.names = FALSE)
```
Looking at all water quality locations
```{r}
water_quality %>% 
  dplyr::select(
    location_id,
    longitude,
    latitude
  ) %>%
  distinct() %>% 
  arrange(desc(latitude))
```


```{r}
water_quality %>% 
  dplyr::select(
    location_id,
    longitude,
    latitude
  ) %>% 
  ggplot(aes(x = longitude, y = latitude))+
  geom_point() + 
  geom_point(
    data = elk,
    aes(x = long, y = lat, color = 'red'), alpha = 0.01
  )
```


Aggregating at the month level. GRTE_SNR01 and GRTE_SNR02 are the two relevent stations to our analysis
```{r}
clean_water = 
  water_quality %>% 
  filter(location_id %in% c('GRTE_SNR01', 'GRTE_SNR02')) %>% 
  mutate(
    result_text = stringr::str_replace(result_text, 'LOW', '1'),
    result_text = stringr::str_replace(result_text, 'ABOVE NORMAL', '3'),
    result_text = stringr::str_replace(result_text, 'NORMAL', '2'),
    result_text = stringr::str_replace(result_text, 'FLOOD', '4'),
  ) |> 
  group_by(
    location_id,
    location_name,
    year,
    month,
    characteristic_name
  ) %>% 
  summarize(
    monthly_mean = mean(as.numeric(result_text),na.rm = TRUE),
    monthly_min = min(as.numeric(result_text),na.rm = TRUE),
    monthly_max = max(as.numeric(result_text),na.rm = TRUE)
  ) %>% 
  pivot_wider(
    names_from = characteristic_name,
    values_from = c('monthly_mean', 'monthly_min', 'monthly_max')
  )
```
```{r}
write.csv(clean_water, '../clean_data/clean_water.csv', row.names = FALSE)
```



## Reading in Temperature

Filter for the four closest stations, filter for the correct date range, average the temperature and snowfall across the stations. 

```{r}
# closest four stations
four_stations <- 
  c("SNAKE RIVER STATION, WY US", "MORAN 5 WNW, WY US", "BURRO HILL WYOMING, WY US", "MOOSE 1 NNE, WY US")

weather = read_csv("../raw_data/raw_weather_data.csv") |> 
  janitor::clean_names() |> 
  filter(
    name %in% four_stations,
    date >= '2006-03-01', 
    date <= '2015-08-25') |> 
  group_by(date) |> 
  summarize(
    tavg = mean(tavg, na.rm = TRUE),
    tmin = mean(tmin, na.rm = TRUE),
    tmax = mean(tmin, na.rm = TRUE),
    prcp = mean(prcp, na.rm = TRUE),
    snow = mean(snow, na.rm = TRUE),
    snwd = mean(snwd, na.rm = TRUE),
  ) |> 
  mutate(
    year = year(date),
    month = month(date),
    day = day(date)
  )
```

Saving clean weather
```{r}
weather |> write.csv('../clean_data/weather', row.names = FALSE)
```



## Combining all data


```{r}
head(elk)
```

```{r}
head(weather)
```

```{r}
head(clean_water)
```


```{r}
all_data = 
  elk %>% 
  left_join(weather |> dplyr::select(-date)) %>% 
  mutate(
    dist_to_GRTE_SNR01 = distHaversine(cbind(long, lat), c(-110.6716, 44.10177))/1000,
    dist_to_GRTE_SNR02 = distHaversine(cbind(long, lat), c(-110.7159, 43.65261))/1000,
    location_id = 
      ifelse(dist_to_GRTE_SNR01 < dist_to_GRTE_SNR02, 'GRTE_SNR01', 'GRTE_SNR02')
  ) %>% 
  left_join(clean_water) %>% 
  dplyr::select(-dist_to_GRTE_SNR01, -dist_to_GRTE_SNR02)
```


```{r}
all_data  |> drop_na(location_name)
```

```{r}
all_data |> write.csv('../clean_data/all_data.csv', row.names =  FALSE)
```


